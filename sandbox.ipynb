{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections Solver Notebook\n",
    "\n",
    "Author: Eric Nunes\n",
    "\n",
    "This notebook is supposed to act as a \"playground\" for me to experiment with different procedures, algorithms, approaches, whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "I built a bot to collect the full game archive from the New York Times (all previous game data is stored in one of their APIs). I'm pulling the dataset from Kaggle, but a mirror exists on HuggingFace.\n",
    "\n",
    "Source:https://www.kaggle.com/datasets/eric27n/the-new-york-times-connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508 {'words': ['snow', 'level', 'shift', 'kayak', 'heat', 'tab', 'bucks', 'return', 'jazz', 'hail', 'option', 'rain', 'sleet', 'racecar', 'mom', 'nets'], 'solution': {'groups': [{'words': ['shift', 'tab', 'return', 'option'], 'reason': 'keyboard keys'}, {'words': ['heat', 'bucks', 'jazz', 'nets'], 'reason': 'nba teams'}, {'words': ['level', 'kayak', 'racecar', 'mom'], 'reason': 'palindromes'}, {'words': ['snow', 'hail', 'rain', 'sleet'], 'reason': 'wet weather'}]}}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"hf://datasets/eric27n/NYT-Connections/Connections_Data.csv\")\n",
    "df['Word'] = df['Word'].fillna(\"NA\")\n",
    "df['Word'] = df['Word'].str.lower()\n",
    "df['Group Name'] = df['Group Name'].str.lower()\n",
    "grouped = df.groupby('Game ID')\n",
    "result = []\n",
    "\n",
    "for game_id, group in grouped:\n",
    "  words = group['Word'].tolist()\n",
    "  group_by_name = group.groupby('Group Name')\n",
    "  solution = []\n",
    "  \n",
    "  for group_name, sub_group in group_by_name:\n",
    "    group_words = sub_group['Word'].tolist()\n",
    "    reason = sub_group['Group Name'].iloc[0]\n",
    "    solution.append({'words': group_words, 'reason': reason})\n",
    "\n",
    "  result.append({'words': words, 'solution': {'groups': solution}})\n",
    "\n",
    "ds = result\n",
    "ds_len = len(ds)\n",
    "print(len(ds), ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Note: One W2V model used is conceptnet-numberbatch. Download the English-only compressed text file from here: https://github.com/commonsense/conceptnet-numberbatch?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'seattle': [('university_of_washington', 0.9806408286094666), ('space_needle', 0.9797334671020508), ('seattleite', 0.9641170501708984), ('emerald_city', 0.9455490112304688), ('tacoma', 0.7643471360206604), ('spokane', 0.7531239986419678), ('portland', 0.7523225545883179), ('lake_chelan', 0.7268684506416321), ('washington', 0.7256752252578735), ('kennewick', 0.7251202464103699)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter\n",
    "import gzip\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "gzipped_file_path = 'numberbatch-en-19.08.txt.gz'\n",
    "decompressed_file_path = 'numberbatch-en-19.08.txt'\n",
    "\n",
    "with gzip.open(gzipped_file_path, 'rt', encoding='utf-8') as f_in:\n",
    "    with open(decompressed_file_path, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "# Import different models\n",
    "model_google = api.load('word2vec-google-news-300')\n",
    "model_glove = api.load('glove-wiki-gigaword-300')\n",
    "model_wiki = api.load('fasttext-wiki-news-subwords-300')\n",
    "model_conceptnet = KeyedVectors.load_word2vec_format(decompressed_file_path, binary=False)\n",
    "\n",
    "# Test: find similar words\n",
    "print(f\"GOOGLE NEWS: {model_google.most_similar('seattle')}\")\n",
    "print(f\"GLOVE: {model_glove.most_similar('seattle')}\")\n",
    "print(f\"WIKI: {model_wiki.most_similar('seattle')}\")\n",
    "print(f\"CONCEPTNET': {model_conceptnet.most_similar('seattle')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract words from ds[i]['words']\n",
    "def guess(model, words):\n",
    "  similarity_matrix = np.zeros((len(words), len(words)))\n",
    "  for i, word1 in enumerate(words):\n",
    "      for j, word2 in enumerate(words):\n",
    "          if word1 in model and word2 in model:\n",
    "              similarity_matrix[i, j] = model.similarity(word1, word2)\n",
    "          else:\n",
    "              similarity_matrix[i, j] = 0\n",
    "\n",
    "  similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "  _max = 0\n",
    "  argmax = 0\n",
    "  argword = \"\"\n",
    "  for idx, word in enumerate(words):\n",
    "      similar_words = similarity_df[word].sort_values(ascending=False)\n",
    "      if similar_words.iloc[1] > _max:\n",
    "        _max = similar_words.iloc[1]\n",
    "        argmax = idx\n",
    "        argword = similar_words.index[1]\n",
    "\n",
    "  build_list = [words[argmax], argword]\n",
    "\n",
    "  words_copy = words.copy()\n",
    "  for test_word in build_list:\n",
    "    words_copy.remove(test_word)\n",
    "\n",
    "  sim_list = []\n",
    "  for test_word in words_copy:\n",
    "    similarities = []\n",
    "    for train_word in build_list:\n",
    "        if train_word in model and test_word in model:\n",
    "            similarity = model.similarity(train_word, test_word)\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            similarities.append(0)  # Handle words not in the model\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    sim_list.append(average_similarity)\n",
    "\n",
    "  index_of_highest_value = sim_list.index(max(sim_list))\n",
    "  build_list.append(words_copy[index_of_highest_value])\n",
    "\n",
    "  words_copy = words.copy()\n",
    "  for test_word in build_list:\n",
    "    words_copy.remove(test_word)\n",
    "\n",
    "  sim_list = []\n",
    "  for test_word in words_copy:\n",
    "    similarities = []\n",
    "    for train_word in build_list:\n",
    "        if train_word in model and test_word in model:\n",
    "            similarity = model.similarity(train_word, test_word)\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            similarities.append(0)  # Handle words not in the model\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    sim_list.append(average_similarity)\n",
    "\n",
    "  index_of_highest_value = sim_list.index(max(sim_list))\n",
    "  build_list.append(words_copy[index_of_highest_value])\n",
    "\n",
    "  return build_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_round(guess_list, solution):\n",
    "  right_count = [0, 0, 0, 0]\n",
    "  for final_word in guess_list:\n",
    "    for idx, group in enumerate(solution['groups']):\n",
    "      if final_word in group['words']:\n",
    "        right_count[idx] += 1\n",
    "  return max(right_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Google News ========\n",
      "AVERAGE SCORE: 2.9664694280078896\n",
      "Google News: First Guess Correctness Distribution\n",
      "1: 10\n",
      "2: 134\n",
      "3: 226\n",
      "4: 137\n",
      "======== Glove ========\n",
      "AVERAGE SCORE: 2.8599605522682445\n",
      "Glove: First Guess Correctness Distribution\n",
      "1: 12\n",
      "2: 173\n",
      "3: 196\n",
      "4: 126\n",
      "======== Wikipedia ========\n",
      "AVERAGE SCORE: 2.9881656804733727\n",
      "Wikipedia: First Guess Correctness Distribution\n",
      "1: 13\n",
      "2: 139\n",
      "3: 196\n",
      "4: 159\n",
      "======== ConceptNet ========\n",
      "AVERAGE SCORE: 3.138067061143984\n",
      "ConceptNet: First Guess Correctness Distribution\n",
      "1: 13\n",
      "2: 95\n",
      "3: 208\n",
      "4: 191\n",
      "Number of Games with At Least One Good First Guess: 285\n"
     ]
    }
   ],
   "source": [
    "models = [model_google, model_glove, model_wiki, model_conceptnet]\n",
    "model_names = [\"Google News\", \"Glove\", \"Wikipedia\", \"ConceptNet\"]\n",
    "correct_idx = []\n",
    "for idx, model in enumerate(models):\n",
    "  print(f\"======== {model_names[idx]} ========\")\n",
    "  right_list = []\n",
    "  one_away_when = []\n",
    "  for i in range(ds_len):\n",
    "    if i == 300:\n",
    "      continue\n",
    "    guess_list = guess(model, ds[i]['words'])\n",
    "    score = eval_round(guess_list, ds[i]['solution'])\n",
    "    right_list.append(score)\n",
    "    if score == 4 and i not in correct_idx:\n",
    "      correct_idx.append(i)\n",
    "\n",
    "  print(f\"AVERAGE SCORE: {sum(right_list) / len(right_list)}\")\n",
    "  print(f\"{model_names[idx]}: First Guess Correctness Distribution\")\n",
    "  for i in range(1, 5):\n",
    "    print(f\"{i}: {right_list.count(i)}\")\n",
    "print(f\"Number of Games with At Least One Good First Guess: {len(correct_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    mean_embedding = embeddings.mean(dim=1).squeeze(0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0][0]\n",
    "\n",
    "def find_most_similar_pair(word_embeddings):\n",
    "    max_similarity = -1\n",
    "    most_similar_pair = (None, None)\n",
    "    words = list(word_embeddings.keys())\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words)):\n",
    "            sim = calculate_similarity(word_embeddings[words[i]], word_embeddings[words[j]])\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                most_similar_pair = (words[i], words[j])\n",
    "\n",
    "    return most_similar_pair\n",
    "\n",
    "def expand_group(selected_words, remaining_words, word_embeddings):\n",
    "    max_similarity = -1\n",
    "    best_word = None\n",
    "\n",
    "    for word in remaining_words:\n",
    "        # Calculate average similarity between the current word and the selected words\n",
    "        similarities = [calculate_similarity(word_embeddings[word], word_embeddings[sw]) for sw in selected_words]\n",
    "        avg_similarity = np.mean(similarities)\n",
    "\n",
    "        if avg_similarity > max_similarity:\n",
    "            max_similarity = avg_similarity\n",
    "            best_word = word\n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_guess(words):\n",
    "    # Generate embeddings for each word\n",
    "    word_embeddings = {word: get_word_embedding(word) for word in words}\n",
    "\n",
    "    # Step 3: Find the pair of two words that are most similar to each other\n",
    "    word1, word2 = find_most_similar_pair(word_embeddings)\n",
    "    selected_words = [word1, word2]\n",
    "\n",
    "    # Step 4-5: Find the next most similar word to the current selection\n",
    "    remaining_words = [w for w in words if w not in selected_words]\n",
    "    for _ in range(2):  # Repeat twice to find 3rd and 4th words\n",
    "        best_word = expand_group(selected_words, remaining_words, word_embeddings)\n",
    "        selected_words.append(best_word)\n",
    "        remaining_words.remove(best_word)\n",
    "\n",
    "    return selected_words\n",
    "\n",
    "def eval_round(words, solution):\n",
    "    right_count = [0, 0, 0, 0]\n",
    "    for final_word in words:\n",
    "        for idx, group in enumerate(solution['groups']):\n",
    "            if final_word in group['words']:\n",
    "                right_count[idx] += 1\n",
    "    return max(right_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100\n",
      "Game 200\n",
      "Game 300\n",
      "Game 400\n",
      "Game 500\n",
      "AVERAGE SCORE: 2.222879684418146\n",
      "========GAMES BY MAX NUM RIGHT ENTRIES========\n",
      "1: 38\n",
      "2: 332\n",
      "3: 123\n",
      "4: 14\n"
     ]
    }
   ],
   "source": [
    "right_list = []\n",
    "for i in range(ds_len):\n",
    "    if i > 0 and i % 100 == 0:\n",
    "        print(f\"Game {i}\")\n",
    "    if i == 300:\n",
    "        continue\n",
    "    words = ds[i]['words']\n",
    "    soln = ds[i]['solution']\n",
    "    optimal_guess = bert_guess(words)\n",
    "    score = eval_round(optimal_guess, soln)\n",
    "    right_list.append(score)\n",
    "\n",
    "print(f\"AVERAGE SCORE: {sum(right_list) / len(right_list)}\")\n",
    "print(\"========GAMES BY MAX NUM RIGHT ENTRIES========\")\n",
    "for i in range(1, 5):\n",
    "    print(f\"{i}: {right_list.count(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...work in progress?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
