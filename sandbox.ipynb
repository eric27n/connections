{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections Solver Notebook\n",
    "\n",
    "Author: Eric Nunes\n",
    "\n",
    "This notebook is supposed to act as a \"playground\" for me to experiment with different procedures, algorithms, approaches, whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "I built a bot to collect the full game archive from the New York Times (all previous game data is stored in one of their APIs). I'm pulling the dataset from Kaggle, but a mirror exists on HuggingFace.\n",
    "\n",
    "Source:https://www.kaggle.com/datasets/eric27n/the-new-york-times-connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYT Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/eric27n/NYT-Connections/Connections_Data.csv\")\n",
    "df['Word'] = df['Word'].fillna(\"NA\")\n",
    "df['Word'] = df['Word'].str.lower()\n",
    "df['Group Name'] = df['Group Name'].str.lower()\n",
    "grouped = df.groupby('Game ID')\n",
    "result = []\n",
    "\n",
    "for game_id, group in grouped:\n",
    "  words = group['Word'].tolist()\n",
    "  group_by_name = group.groupby('Group Name')\n",
    "  solution = []\n",
    "  \n",
    "  for group_name, sub_group in group_by_name:\n",
    "    group_words = sub_group['Word'].tolist()\n",
    "    reason = sub_group['Group Name'].iloc[0]\n",
    "    solution.append({'words': group_words, 'reason': reason})\n",
    "\n",
    "  result.append({'words': words, 'solution': {'groups': solution}})\n",
    "\n",
    "ds = result\n",
    "ds_len = len(ds)\n",
    "print(len(ds), ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Connect (Datathon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"\"\"\n",
    "[\"Cannon\",\"Santa\",\"Fury\",\"Jonas\"]\t People named ‘Nick'\n",
    "[\"Love\",\"Zip\",\"Duck\",\"Goose-egg\"] \tTerms for 'zero'\n",
    "[\"Haddock\",\"Ahab\",\"Hook\",\"Sparrow\"] \tFictional captains\n",
    "[\"Ash\",\"Lime\",\"Willow\",\"Cork\"]\t Trees\n",
    "\n",
    "[\"Bush\",\"Dole\",\"Gore\",\"Carter\"]\tUS presidential losers\n",
    "[\"Mexico\",\"Hampshire\",\"Jersey\",\"York\"]\t'New' US states\n",
    "[\"Glandular\",\"Scarlet\",\"Yellow\",\"Trench\"]\tTypes of fever\n",
    "[\"Eastern\",\"Central\",\"Mountain\",\"Pacific\"]\tUSA Time Zones\n",
    "\n",
    "[\"Manhattan\",\"Sidecar\",\"Margarita\",\"Gibson\"]\tCocktails\n",
    "[\"Puzzle\",\"Business\",\"Nuts\",\"Suit\"]\tMonkey ____\n",
    "[\"Fruit\",\"Carrot\",\"Sheet\",\"Marble\"]\tCakes\n",
    "[\"Dough\",\"Bands\",\"Scratch\",\"Bread\"]\t  Slang for money\n",
    "\n",
    "[\"Magpie\",\"Crane\",\"Turkey\",\"Black\"]\tBirds\n",
    "[\"Passion\",\"Kiwi\",\"Jack\",\"Grape\"]\tFruit\n",
    "[\"Gold\",\"Yellow\",\"Jungle\",\"Scarlet\"]\tFever\n",
    "[\"Hawk\",\"Cruise\",\"Trident\",\"Patriot\"]\tMissiles\n",
    "\n",
    "[\"Anvil\",\"Hammer\",\"Stirrup\",\"Tympanum\"]\tEar parts\n",
    "[\"Square\",\"Cube\",\"Irrational\",\"Perfect\"]\tTypes of numbers\n",
    "[\"Presto\",\"Largo\",\"Piano\",\"Grave\"]\tMusical directions\n",
    "[\"Altering\",\"Triangle\",\"Relating\",\"Integral\"]\tAnagrams of each other\n",
    "\n",
    "[\"Queue\",\"Pea\",\"Jay\",\"Sea\"]\tSound like letters of the alphabet\n",
    "[\"Lime\",\"Emerald\",\"Jade\",\"Olive\"]\tShades of green\n",
    "[\"Warner\",\"Wright\",\"Chemical\",\"Moss\"]\t____ Brothers\n",
    "[\"Cardinal\",\"Burgundy\",\"Venetian\",\"Rust\"]\tShades of red\n",
    "\n",
    "[\"Coffee\",\"Terrier\",\"Yew\",\"Rover\"]\tIrish ____\n",
    "[\"Boxer\",\"Whiskey\",\"Mau Mau\",\"Easter\"]\tRebellions\n",
    "[\"Chekov\",\"Kirk\",\"McCoy\",\"Scott\"]\t'Star Trek' characters\n",
    "[\"Echo\",\"Explorer\",\"Voyager\",\"Sputnik\"]\tSpacecraft\n",
    "\n",
    "[\"Foxtrot\",\"Tango\",\"Tap\",\"Morris\"]\tDances\n",
    "[\"Clover\",\"Napoleon\",\"Major\",\"Boxer\"]\t'Animal Farm' characters\n",
    "[\"Naan\",\"Hannah\",\"Level\",\"Peep\"]\tPalindromes\n",
    "[\"Honey\",\"Mason\",\"Bumble\",\"Mining\"]\tVarieties of bee\n",
    "\n",
    "[\"Rabbit\",\"Knife\",\"Hammer\",\"Ass\"]\tJack____\n",
    "[\"Question\",\"Pot\",\"Gun\",\"Employee\"]\tThings you can 'fire'\n",
    "[\"Pencil\",\"Bubble\",\"Tulip\",\"Hobble\"]\tStyles of skirt\n",
    "[\"Cruiser\",\"Mountain\",\"Tandem\",\"BMX\"]\tTypes of bicycles\n",
    "\n",
    "[\"Stag\",\"Tom\",\"Bull\",\"Buck\"]\tMale animals\n",
    "[\"Crumb\",\"Column\",\"Debris\",\"Crochet\"]\tEnd with a silent letter\n",
    "[\"Big Bang\",\"Chaos\",\"String\",\"Game\"]\tScientific theories\n",
    "[\"Coming\",\"Hand\",\"Childhood\",\"Guess\"]\tSecond ____\n",
    "\n",
    "[\"Whip\",\"Smile\",\"Joke\",\"Bottle\"]\tThings you can crack\n",
    "[\"Baby\",\"Concert\",\"Ballroom\",\"Parlour\"]\tSizes of grand pianos\n",
    "[\"Con\",\"Fleece\",\"Rook\",\"Screw\"]\tSynonyms for swindle\n",
    "[\"Precious\",\"Love\",\"Petal\",\"Honey\"]\tTerms of endearment\n",
    "\n",
    "[\"Base\",\"Heavy\",\"Scrap\",\"Noble\"]\t____ metal\n",
    "[\"Mace\",\"Saffron\",\"Nigella\",\"Ginger\"]\tSpices\n",
    "[\"Screw\",\"Spit\",\"Fast\",\"Curve\"]\tBaseball pitches\n",
    "[\"Python\",\"Logo\",\"Perl\",\"C\"]\tComputer programming languages\n",
    "\n",
    "[\"Marks\",\"Last legs\",\"Knees\",\"Feet\"]\tOn your ____\n",
    "[\"Jackson\",\"Austin\",\"Carson\",\"Pierre\"]\tUS state capitals and male first names\n",
    "[\"Dewey\",\"Louie\",\"Daisy\",\"Donald\"]\tDisney ducks\n",
    "[\"Control\",\"Escape\",\"Insert\",\"Function\"]\tPC computer keys\n",
    "\n",
    "[\"Coal\",\"Pipe\",\"Scarf\",\"Carrot\"]\tAccessories for a snowman\n",
    "[\"Jester\",\"Cow\",\"Sleigh\",\"Church\"]\tThings that have bells\n",
    "[\"Diamond\",\"Hound\",\"Oath\",\"Bath\"]\tBlood ____\n",
    "[\"Priest\",\"Bishop\",\"Pastor\",\"Cantor\"]\tReligious offices\n",
    "\n",
    "[\"Power\",\"Harmonic\",\"Fourier\",\"Telescoping\"]\tClasses of mathematical series\n",
    "[\"Okay\",\"Eighty\",\"Tepee\",\"Excess\"]\tPronounced as two letters\n",
    "[\"Jellyfish\",\"Snake\",\"Algae\",\"Mushroom\"]\tThings with poisonous varieties\n",
    "[\"Grape\",\"Ivy\",\"Clematis\",\"Wisteria\"]\tTypes of vines\n",
    "\n",
    "[\"Scar\",\"Hades\",\"Stromboli\",\"Ursula\"]\tDisney animated villains\n",
    "[\"Doctor\",\"Hazel\",\"Hunt\",\"Craft\"]\tWitch ____\n",
    "[\"Doll\",\"Bird\",\"Dame\",\"Sheila\"]\tNicknames for women\n",
    "[\"Fighting\",\"Grass\",\"Dragon\",\"Psychic\"]\tTypes of Pokémon\n",
    "\n",
    "[\"Key\",\"Ribbon\",\"Platen\",\"Carriage\"]\tParts of a typewriter\n",
    "[\"Sheffield United\",\"Razor\",\"Helicopter\",\"Ice skate\"]\tBlades\n",
    "[\"Bottle\",\"Suicide\",\"Dumb\",\"Legally\"]\t____ blonde\n",
    "[\"Coin\",\"Spider's web\",\"Cricket ball\",\"Good yarn\"]\tThings that are spun\n",
    "\n",
    "[\"Honey\",\"Cox\",\"Nit\",\"Curry\"]\tTypes of comb\n",
    "[\"Vega\",\"Betelgeuse\",\"Pollux\",\"Altair\"]\tStars\n",
    "[\"10\",\"Pitch\",\"Tense\",\"Number\"]\tPerfect ____\n",
    "[\"Blood\",\"Tyre\",\"Iron\",\"Shotgun\"]\tThings that can be pumped\n",
    "\n",
    "[\"Sacrifice\",\"Pin\",\"Fork\",\"Discovered attack\"]\tChess tactics\n",
    "[\"Rocking\",\"Carver\",\"Windsor\",\"Tub\"]\tChairs\n",
    "[\"Brilliant\",\"Emerald\",\"Baguette\",\"Pear\"]\tCuts of diamond\n",
    "[\"Swing\",\"Arch\",\"Beam\",\"Pontoon\"]\tBridges\n",
    "\n",
    "[\"Worm\",\"Trojan horse\",\"Virus\",\"Keylogger\"]\tMalware\n",
    "[\"Fandango\",\"Thunderbolt\",\"Beelzebub\",\"Galileo\"]\tLyrics in 'Bohemian Rhapsody'\n",
    "[\"Torpedo\",\"Hoagie\",\"Poor boy\",\"Submarine\"]\tUS sandwiches\n",
    "[\"Viper\",\"Maverick\",\"Hollywood\",\"Iceman\"]\tPilots in 'Top Gun'\n",
    "\n",
    "[\"Rotary\",\"Fight\",\"Mile High\",\"Yacht\"]\tFamous clubs\n",
    "[\"Dragon\",\"Fiona\",\"Puss in Boots\",\"Harold\"]\tCharacters in Shrek films\n",
    "[\"Leech\",\"Fluke\",\"Tick\",\"Chigger\"]\tParasites\n",
    "[\"Wrecking\",\"Medicine\",\"Masked\",\"Crystal\"]\t____ Ball\n",
    "\n",
    "[\"Bow\",\"Plough\",\"Hook\",\"Drogue\"]\tTypes of anchor\n",
    "[\"Tangled\",\"Mulan\",\"Cinderella\",\"Fantasia\"]\tDisney animations\n",
    "[\"Police\",\"Steam\",\"Slide\",\"Penny\"]\tTypes of whistle\n",
    "[\"Dakota\",\"Shields\",\"Korea\",\"Atlantic\"]\tNorth and South ____\n",
    "\n",
    "[\"Beard\",\"Bath\",\"Shrewd\",\"Harem\"]\tMammals plus one letter\n",
    "[\"Air\",\"Street\",\"Ice\",\"Field\"]\tTypes of hockey\n",
    "[\"World\",\"64\",\"Land\",\"Bros.\"]\t'Super Mario' games\n",
    "[\"Radar\",\"Table\",\"Sea\",\"Weather\"]\tUnder the ____\n",
    "\n",
    "[\"Garter\",\"Confetti\",\"Bouquet\",\"Petals\"]\tThrown at weddings\n",
    "[\"Zuckerberg\",\"Filo\",\"Wales\",\"Page\"]\tInternet entrepreneurs\n",
    "[\"Jet\",\"Bill\",\"Raven\",\"Saint\"]\tMembers of American football teams\n",
    "[\"Won\",\"Tu\",\"Fore\",\"Ait\"]\tNumber homophones\n",
    "\n",
    "[\"Boycott\",\"Walkout\",\"Picket\",\"Rally\"]\tTypes of protest\n",
    "[\"Demolition\",\"Medicine\",\"Masked\",\"Crystal\"]\t____ ball\n",
    "[\"Red\",\"Tiananmen\",\"Times\",\"Trafalgar\"]\tWorld-famous city squares\n",
    "[\"Brian\",\"Stewie\",\"Cleveland\",\"Meg\"]\t'Family Guy' characters\n",
    "\n",
    "[\"Snorlax\",\"Jigglypuff\",\"Bulbasaur\",\"Ditto\"]\tPokémon characters\n",
    "[\"Rare\",\"Well done\",\"Medium\",\"Blue\"]\tTemperatures for cooking beef\n",
    "[\"Shelf\",\"Drift\",\"Plate\",\"Crust\"]\tContinental ____\n",
    "[\"Dubrovnik\",\"Split\",\"Rijeka\",\"Zadar\"]\tCities in Croatia\n",
    "\n",
    "[\"Achtung Baby\",\"Zooropa\",\"War\",\"Rattle and Hum\"]\tU2 albums\n",
    "[\"Yosemite\",\"Redwood\",\"Joshua Tree\",\"Death Valley\"]\tNational Parks in California\n",
    "[\"Eden\",\"Babylon\",\"Versailles\",\"Monet\"]\tFamous gardens\n",
    "[\"Angry Birds\",\"Vine\",\"Instagram\",\"Fruit Ninja\"]\tPopular apps\n",
    "\n",
    "[\"Iron Man\",\"Daredevil\",\"Hulk\",\"Blade\"]\tMarvel Comics superheroes\n",
    "[\"Determined\",\"Honest\",\"Sure\",\"Frank\"]\tTo be ____\n",
    "[\"Castle\",\"Gambit\",\"Stalemate\",\"En passant\"]\tChess terms\n",
    "[\"Corral\",\"Assemble\",\"Muster\",\"Convene\"]\tGather people together\n",
    "\n",
    "[\"Colorado\",\"Missouri\",\"Green\",\"Arkansas\"]\tMajor US rivers\n",
    "[\"Lapel\",\"Cuff\",\"Breast\",\"Pocket\"]\tParts of a jacket\n",
    "[\"Ross\",\"Monk\",\"Elephant\",\"Harbour\"]\tSpecies of seal\n",
    "[\"Brown\",\"Columbia\",\"Cornell\",\"Yale\"]\tIvy League establishments\n",
    "\n",
    "[\"Merida\",\"Pocahontas\",\"Jasmine\",\"Snow White\"]\tDisney Princesses\n",
    "[\"Yellow\",\"Sparks\",\"Viva La Vida\",\"Paradise\"]\tColdplay songs\n",
    "[\"Thick\",\"Raw\",\"Altogether\",\"Cut\"]\tIn the ____\n",
    "[\"Saratoga\",\"Iowa\",\"Missouri\",\"Duffy\"]\tUS ships of World War II\n",
    "\n",
    "[\"Radiation\",\"Altitude\",\"Morning\",\"Motion\"]\tForms of sickness\n",
    "[\"Turnover\",\"Strudel\",\"Cobbler\",\"Crumble\"]\tApple dishes\n",
    "[\"Off the Wall\",\"Invincible\",\"Thriller\",\"Dangerous\"]\tMichael Jackson albums\n",
    "[\"Pigwidgeon\",\"Hermes\",\"Hedwig\",\"Errol\"]\tHarry Potter owls\n",
    "\n",
    "[\"Bloody\",\"Virgin\",\"Typhoid\",\"Hail\"]\t___ Mary\n",
    "[\"Rare\",\"Medium\",\"Well done\",\"Blue\"]\tMeat cooking times\n",
    "[\"Nobel\",\"Carver\",\"Japp\",\"Franklin\"]\tChemists\n",
    "[\"Malik\",\"Styles\",\"Horan\",\"Payne\"]\tOne Direction\n",
    "\n",
    "[\"Cat\",\"Sleep\",\"Moon\",\"Cake\"]\t____walk\n",
    "[\"Midtown\",\"Hell's Kitchen\",\"NoHo\",\"Chelsea\"]\tManhattan\n",
    "[\"Orache\",\"Escarole\",\"Kale\",\"Bok Choy\"]\tLeafy green vegetables\t\n",
    "[\"Use\",\"Wise\",\"Pease\",\"Seize\"]\tHomophones of letter plurals\n",
    "\n",
    "[\"Time\",\"Skin\",\"Face\",\"Breath\"]\tThings that can be saved\t\n",
    "[\"Fairy\",\"Puck\",\"Imp\",\"Sprite\"]\tSupernatural creatures\t\n",
    "[\"Neymar\",\"Silva\",\"Sócrates\",\"Vinicius\"]\tBrazilian international footballers\t\n",
    "[\"Dream\",\"Sniper\",\"Hustle\",\"Pie\"]\tAmerican ___\n",
    "\n",
    "[\"Debris\",\"Coup\",\"Autumn\",\"Crumb\"]\tEnd with a silent letter\n",
    "[\"Clooney\",\"Pitt\",\"Cheadle\",\"Mac\"]\t'Ocean's' actors\n",
    "[\"Windward\",\"Virgin\",\"Canary\",\"Faroe\"]\tIsland groups\n",
    "[\"Overwatch\",\"Destiny\",\"Halo\",\"Doom\"]\tFirst-person shooters\n",
    "\n",
    "[\"Fingerboard\",\"Neck\",\"Bridge\",\"Pickup\"]\tParts of an electric guitar\n",
    "[\"Blurred Lines\",\"Get Lucky\",\"Happy\",\"Angel\"]\tPharrell Williams songs\n",
    "[\"Frontier\",\"Countdown\",\"Fantasy\",\"Score\"]\tFinal  ____\n",
    "[\"Note\",\"Micra\",\"Cube\",\"Atlas\"]\tNissan car models\n",
    "\n",
    "[\"Patriot\",\"Charger\",\"Cowboy\",\"Saint\"]\tNFL Teams\n",
    "[\"Tucker\",\"Barbie\",\"Yakka\",\"Sheila\"]\tAustralian slang terms\n",
    "[\"Grip\",\"Stock\",\"Butt\",\"Barrel\"]\tParts of a rifle\n",
    "[\"Flick\",\"Bowie\",\"Stanley\",\"Cheese\"]\tKnives\n",
    "\n",
    "[\"Wraith\",\"Phantom\",\"Cullinan\",\"Silver Ghost\"]\tRolls Royce models\n",
    "[\"Trouble\",\"Franco\",\"Neuron\",\"Boulevard\"]\tContain currencies\n",
    "[\"Corpus Christi\",\"Amarillo\",\"Houston\",\"Austin\"]\tTexan towns\n",
    "[\"1977\",\"Toaster\",\"Valencia\",\"Rio de Janeiro\"]\tInstagram filters\n",
    "\n",
    "[\"Reeve\",\"Cavill\",\"Cain\",\"Routh\"]\tPlayed Superman\n",
    "[\"False\",\"Robin\",\"Neighbor\",\"Knight\"]\t____hood\n",
    "[\"Bewilder\",\"Puzzle\",\"Stump\",\"Baffle\"]\tWords meaning to perplex\n",
    "[\"National\",\"Piano\",\"Cru\",\"Canyon\"]\tGrand ____\n",
    "\n",
    "[\"Indian\",\"Triumph\",\"Ducati\",\"Aprilia\"]\tMotorcycle brands\n",
    "[\"Grisham\",\"Slaughter\",\"King\",\"Child\"]\tThriller authors\n",
    "[\"Crown\",\"Amalgam\",\"Calculus\",\"Root\"]\tDentistry terms\n",
    "[\"Cookie\",\"Mason\",\"Bell\",\"Leyden\"]\tTypes of jar\n",
    "\n",
    "[\"Cottonmouth\",\"Rattle\",\"Corn\",\"Rat\"]\tSnakes\n",
    "[\"Money\",\"Clip\",\"Trail\",\"Towns\"]\tPaper _____\n",
    "[\"Not\",\"Night\",\"Nave\",\"Nob\"]\tLost their initial k\n",
    "[\"Deal\",\"Livers\",\"Nit\",\"Snug tent\"]\tAnagrams of metals\n",
    "\n",
    "[\"Constantine\",\"The Matrix\",\"Speed\",\"John Wick\"]\tKeanu Reeves films\n",
    "[\"Sea lion\",\"Dugong\",\"Narwhal\",\"Manatee\"]\tMarine mammals\n",
    "[\"Fu Manchu\",\"Handlebar\",\"Pencil\",\"Walrus\"]\t Mustaches\n",
    "[\"Saddle\",\"Pedal\",\"Seat post\",\"Fork\"]\tParts of a bicycle\n",
    "\n",
    "[\"The Big Short\",\"Half Nelson\",\"Fracture\",\"La La Land\"]\tRyan Gosling films\n",
    "[\"Bet\",\"Ante\",\"Flutter\",\"Wager\"]\tGamble\n",
    "[\"Say You Will\",\"Tusk\",\"Mirage\",\"Penguin\"]\tFleetwood Mac albums\n",
    "[\"Rager\",\"Blue Sky\",\"Brightside\",\"Morale\"]\tSongs called 'Mr ____'\n",
    "\n",
    "[\"Cave\",\"Performance\",\"Pop\",\"Kinetic\"]\tTypes of art\t\n",
    "[\"United\",\"Delta\",\"Frontier\",\"Spirit\"]\tUS airlines\t\n",
    "[\"Milk\",\"Puzzle\",\"Measuring\",\"Claret\"]\tJugs\t\n",
    "[\"Boston\",\"Nantucket\",\"Quincy\",\"Springfield\"]\tPlaces in Massachusetts\t\n",
    "\n",
    "[\"Lakeland\",\"Kerry Blue\",\"Border\",\"Yorkshire\"]\tTerriers\t\n",
    "[\"Knuckle\",\"Wedding\",\"Flag\",\"Trash\"]\tWhite ____\t\n",
    "[\"Drummer\",\"Maid\",\"Partridge\",\"Swan\"]\tTwelve Days of Christmas\t\n",
    "[\"Sesame\",\"Palm\",\"Sunflower\",\"Argan\"]\tEdible oils\t\n",
    "\n",
    "[\"Headon\",\"Judd\",\"Fleetwood\",\"Bonham\"]\tDrummers\t\n",
    "[\"Work\",\"Place\",\"Wall\",\"Power\"]\tFire____\t\n",
    "[\"Photograph\",\"Chance\",\"Bow\",\"Back seat\"]\tThings you can take\t\n",
    "[\"Rock\",\"Pine\",\"Evert\",\"Jericho\"]\tFamous people named Chris\t\n",
    "\n",
    "[\"Junk\",\"Royal\",\"Chain\",\"Fan\"]\t____ mail\t\n",
    "[\"Troy\",\"Chad\",\"Gabriella\",\"Taylor\"]\t'High School Musical' characters\t\n",
    "[\"Barnacles\",\"Peso\",\"Kwazii\",\"Inkling\"]\tOctonauts\t\n",
    "[\"Golf\",\"Cricket\",\"Rugby\",\"Polo\"]\tBall games\t\n",
    "\n",
    "[\"up!\",\"Amarok\",\"Polo\",\"Jetta\"]\tVolkswagens\t\n",
    "[\"Tightrope\",\"Dog\",\"Walk\",\"Line\"]\tWalk the ____\t\n",
    "[\"Safety\",\"Cornerback\",\"Tackle\",\"Center\"]\tPositions in American football\t\n",
    "[\"Marker\",\"Ballpoint\",\"Fountain\",\"Dip\"]\tTypes of pen\t\n",
    "\n",
    "[\"Piquet\",\"Hill\",\"Scheckter\",\"Ascari\"]\tFormula One champions\t\n",
    "[\"Vietnam\",\"New Mexico\",\"Soviet Union\",\"Catalonia\"]\tRed and gold flags\t\n",
    "[\"Marshall\",\"Skye\",\"Rubble\",\"Chase\"]\t'PAW Patrol' characters\t\n",
    "[\"Coriolanus\",\"Haymitch\",\"Peeta\",\"Katniss\"]\t'The Hunger Games' characters\t\n",
    "\n",
    "[\"Rose\",\"Winks\",\"Kane\",\"Walker-Peters\"]\tTottenham Hotspur players\t\n",
    "[\"Fury\",\"Whyte\",\"Haye\",\"Chisora\"]\tBritish heavyweight boxers\t\n",
    "[\"Case\",\"Egg\",\"Joke\",\"Knuckles\"]\tThings you crack\t\n",
    "[\"Snare\",\"Crash\",\"Tom\",\"Ride\"]\tParts of a drum kit\t\n",
    "\n",
    "[\"Iron\",\"Stan\",\"Grandpa\",\"Ben\"]\tFictional animated uncles\t\n",
    "[\"Wisdom\",\"False\",\"Milk\",\"Eye\"]\t____ teeth\n",
    "[\"Phoenix\",\"Stone\",\"Prince\",\"Fire\"]\tLast words of Harry Potter book titles\n",
    "[\"Labour\",\"Coach\",\"Rescue\",\"Birthday\"]\tTypes of party\n",
    "\n",
    "[\"George Bush\",\"Dulles\",\"Douglas\",\"Sky Harbor\"]\tUS airports\n",
    "[\"Barnum\",\"Memphis\",\"Van Helsing\",\"Valjean\"]\tPlayed by Hugh Jackman\n",
    "[\"Grape\",\"Rango\",\"Wood\",\"Brasco\"]\tJohnny Depp title roles\n",
    "[\"Juliet\",\"Antigone\",\"Brunnhilde\",\"Javert\"]\tFictional suicides\n",
    "\n",
    "[\"Sonora\",\"Chihuahua\",\"Yukatán\",\"Tabasco\"]\tMexican states\n",
    "[\"Asuncion\",\"Paramaribo\",\"Lima\",\"Quito\"]\tSouth American capitals\n",
    "[\"Boxster\",\"Panamera\",\"Cayman\",\"Cayenne\"]\tPorsche vehicles\n",
    "[\"Vervet\",\"Woolly\",\"Spider\",\"Proboscis\"]\tMonkeys\n",
    "\n",
    "[\"Cavendish\",\"Froom\",\"Kenny\",\"Varnish\"]\tBritish cyclists\n",
    "[\"Stout\",\"Bitter\",\"Mild\",\"Bock\"]\tBeer\n",
    "[\"Meade\",\"Grant\",\"Sherman\",\"Beauregard\"]\tUS Civil War generals\n",
    "[\"Sirloin\",\"Skirt\",\"T Bone\",\"Picanha\"]\tSteaks\n",
    "\n",
    "[\"Bonsai\",\"Honcho\",\"Sake\",\"Emoji\"]\tJapanese words\n",
    "[\"Strong nuclear\",\"Electromagnetic\",\"Elastic\",\"Friction\"]\tForces\n",
    "[\"Lumbers\",\"Jib\",\"Rush\",\"Judged\"]\tBooks of the Bible with a letter changed\n",
    "[\"Poppadom\",\"Roti\",\"Bhaji\",\"Pakora\"]\tIndian food\n",
    "\n",
    "[\"Amarillo\",\"Rosa\",\"Verde\",\"Azul\"]\tSpanish colours\n",
    "[\"Auror\",\"Moan\",\"Tian\",\"Els\"]\tDisney princesses minus final \"a\"\n",
    "[\"Rain\",\"Joaquin\",\"Summer\",\"Liberty\"]\tPhoenix siblings\n",
    "[\"World\",\"Food\",\"Sperm\",\"River\"]\t____ Bank\n",
    "\n",
    "[\"Wisdom\",\"False\",\"Milk\",\"Eye\"]\t____ teeth\n",
    "[\"Phoenix\",\"Stone\",\"Prince\",\"Fire\"]\tLast words of Harry Potter book titles\n",
    "[\"Labour\",\"Coach\",\"Rescue\",\"Birthday\"]\tTypes of party\n",
    "[\"George Bush\",\"Dulles\",\"Douglas\",\"Sky Harbor\"]\tUS airports\n",
    "\n",
    "[\"Barnum\",\"Memphis\",\"Van Helsing\",\"Valjean\"]\tPlayed by Hugh Jackman\n",
    "[\"Grape\",\"Rango\",\"Wood\",\"Brasco\"]\tJohnny Depp title roles\n",
    "[\"Juliet\",\"Antigone\",\"Brunnhilde\",\"Javert\"]\tFictional suicides\n",
    "[\"Sonora\",\"Chihuahua\",\"Yukatán\",\"Tabasco\"]\tMexican states\n",
    "\n",
    "[\"Asuncion\",\"Paramaribo\",\"Lima\",\"Quito\"]\tSouth American capitals\n",
    "[\"Boxster\",\"Panamera\",\"Cayman\",\"Cayenne\"]\tPorsche vehicles\n",
    "[\"Vervet\",\"Woolly\",\"Spider\",\"Proboscis\"]\tMonkeys\n",
    "[\"Cavendish\",\"Froom\",\"Kenny\",\"Varnish\"]\tBritish cyclists\n",
    "\n",
    "[\"Stout\",\"Bitter\",\"Mild\",\"Bock\"]\tBeer\n",
    "[\"Meade\",\"Grant\",\"Sherman\",\"Beauregard\"]\tUS Civil War generals\n",
    "[\"Sirloin\",\"Skirt\",\"T Bone\",\"Picanha\"]\tSteaks\n",
    "[\"Bonsai\",\"Honcho\",\"Sake\",\"Emoji\"]\tJapanese words\n",
    "\n",
    "[\"Strong nuclear\",\"Electromagnetic\",\"Elastic\",\"Friction\"]\tForces\n",
    "[\"Lumbers\",\"Jib\",\"Rush\",\"Judged\"]\tBooks of the Bible with a letter changed\n",
    "[\"Poppadom\",\"Roti\",\"Bhaji\",\"Pakora\"]\tIndian food\n",
    "[\"Amarillo\",\"Rosa\",\"Verde\",\"Azul\"]\tSpanish colours\n",
    "\n",
    "[\"Auror\",\"Moan\",\"Tian\",\"Els\"]\tDisney princesses minus final \"a\"\n",
    "[\"Rain\",\"Joaquin\",\"Summer\",\"Liberty\"]\tPhoenix siblings\n",
    "[\"World\",\"Food\",\"Sperm\",\"River\"]\t____ Bank\n",
    "[\"Rush Hour\",\"Vacation\",\"Confessions\",\"House Party\"]\tComedy film series\n",
    "\n",
    "[\"Peking\",\"The Fat\",\"Rubber\",\"Milkshake\"]\t____ Duck\n",
    "[\"Canyon\",\"Gorge\",\"Vale\",\"Dene\"]\tValleys\n",
    "[\"Cram\",\"Wolf\",\"Scoff\",\"Bolt\"]\tEat quickly\n",
    "[\"Lance\",\"Neil\",\"Alexander\",\"Stretch\"]\tArmstrongs\n",
    "\n",
    "[\"Tax\",\"Extend\",\"Push\",\"Challenge\"]\tAsk for effort\n",
    "[\"Surfer\",\"Lining\",\"Service\",\"Birch\"]\tSilver ____\n",
    "[\"Torr\",\"Atmosphere\",\"PSI\",\"Pascal\"]\tUnits of pressure\n",
    "[\"Po\",\"Tiber\",\"Arno\",\"Passer\"]\tItalian rivers\n",
    "\n",
    "[\"Heat\",\"Magic\",\"Thunder\",\"Celtics\"]\tUS basketball teams\n",
    "[\"God\",\"Art\",\"Dogs\",\"Masters\"]\t____ of War\n",
    "[\"Hunt\",\"Fish\",\"Boat\",\"Drive\"]\tNeed a license\n",
    "[\"Marquez\",\"Rossi\",\"Lorenzo\",\"Stoner\"]\tMotoGP champions\n",
    "\n",
    "[\"Cello\",\"Balalaika\",\"Tambura\",\"Harp\"]\tString instruments\n",
    "[\"The Lovers\",\"Wheel of Fortune\",\"Justice\",\"Strength\"]\tTarot cards\n",
    "[\"Subsistence\",\"Arable\",\"Intensive\",\"Dairy\"]\tTypes of farming\n",
    "[\"Coley\",\"Char\",\"Cod\",\"Cusk\"]\tFish\n",
    "\n",
    "[\"Chad\",\"Cameroon\",\"Comoros\",\"Cabo Verde\"]\tAfrican countries\n",
    "[\"Tito\",\"Castro\",\"Pot\",\"Stoph\"]\tCommunist leaders\n",
    "[\"Pole\",\"Shooting\",\"Dwarf\",\"Evening\"]\t____ star\n",
    "[\"Hobbit\",\"Elf\",\"Orc\",\"Warg\"]\tTolkien creatures\n",
    "\n",
    "[\"Victorinox\",\"Swatch\",\"UBS\",\"Schindler\"]\tSwiss companies\n",
    "[\"Intertwine\",\"Origin\",\"Decider\",\"Whale\"]\tEnd in alcoholic drinks\n",
    "[\"Gneiss\",\"Marble\",\"Schist\",\"Slate\"]\tTypes of rock\n",
    "[\"Rock\",\"Dundee\",\"Clip\",\"Tears\"]\tCrocodile ____\n",
    "\n",
    "[\"Cosmopolitan\",\"Vesper\",\"Caipiroska\",\"Kamikaze\"]\tVodka cocktails\n",
    "[\"Peaches\",\"Crash\",\"Scrat\",\"Manny\"]\t'Ice Age' characters\n",
    "[\"Spotify\",\"Warthog\",\"Molecule\",\"Zither\"]\tStart with skin blemishes\n",
    "[\"Abraham\",\"Christmas\",\"Damien\",\"Brown\"]\tFather ____\n",
    "\n",
    "[\"Major\",\"Peel\",\"Eden\",\"Heath\"]\tBritish Prime Ministers\n",
    "[\"Marathi\",\"Malayalam\",\"Telugu\",\"Odia\"]\tIndian languages\n",
    "[\"Heimdall\",\"Fenrir\",\"Loki\",\"Odin\"]\tNorse mythology\n",
    "[\"Young\",\"Bean\",\"Lock\",\"Combs\"]\tSeans\n",
    "\n",
    "[\"Villanelle\",\"Ode\",\"Haiku\",\"Sonnet\"]\tPoetic forms\n",
    "[\"Vishnu\",\"Kali\",\"Ganesha\",\"Brahma\"]\tHindu deities\n",
    "[\"Shiva\",\"Richard Parker\",\"Tigger\",\"Hobbes\"]\tFictional tigers\n",
    "[\"Villa\",\"Basilica\",\"Forum\",\"Circus\"]\tRoman buildings\n",
    "\n",
    "[\"Number\",\"Time\",\"Meridian\",\"Suspect\"]\tPrime ____\n",
    "[\"Ratatouille\",\"Coco\",\"Brave\",\"Onward\"]\tPixar films\n",
    "[\"Tan\",\"Sec\",\"Cosec\",\"Cot\"]\tTrigonometric functions\n",
    "[\"Nemo\",\"Mrs Puff\",\"Klaus\",\"Flounder\"]\tAnimated fish\n",
    "\n",
    "[\"Smollett\",\"Ahab\",\"Haddock\",\"Handy\"]\tFictional sea captains\n",
    "[\"Orzoi\",\"Oxer\",\"Eagle\",\"Asset\"]\tDogs missing an initial \"B\"\n",
    "[\"Beer\",\"Mothering\",\"Anthrax\",\"Mantissa\"]\tStart with insects\n",
    "[\"Malinga\",\"Sangakkara\",\"Mathews\",\"Arnold\"]\tSri Lankan cricketers\n",
    "\n",
    "[\"Dog\",\"Slide\",\"Pea\",\"Tin\"]\tTypes of whistle\n",
    "[\"Fat\",\"Lanca\",\"Ferrar\",\"Pagan\"]\tItalian car manufacturers missing \"i\"\n",
    "[\"Corfu\",\"Kos\",\"Samos\",\"Santorini\"]\tGreek islands\n",
    "[\"Io\",\"Callisto\",\"Ganymede\",\"Kale\"]\tMoons of Jupiter\n",
    "\n",
    "[\"Scott\",\"Poehler\",\"Plaza\",\"Ansari\"]\t'Parks and Recreation' actors\n",
    "[\"Tuba\",\"Sousaphone\",\"Shofar\",\"Cornet\"]\tBrass instruments\n",
    "[\"Benning\",\"Wayne\",\"Sumter\",\"Lauderdale\"]\tFort ____ in US\n",
    "[\"Smother\",\"Carbuncle\",\"Season\",\"Flaunt\"]\tEnds with a relative\n",
    "\n",
    "[\"Safari\",\"Clock\",\"Compass\",\"Notes\"]\tDefault iPhone apps\n",
    "[\"Carbonara\",\"Tintoretto\",\"Neonate\",\"Leadsom\"]\tBegin with an element\n",
    "[\"X\",\"Plum\",\"Branestawm\",\"Layton\"]\tProfessor ____\n",
    "[\"United\",\"Rangers\",\"Athletic\",\"Orient\"]\tLondon football club suffixes\n",
    "\n",
    "[\"Operation\",\"Battleship\",\"Twister\",\"Mastermind\"]\tGames\n",
    "[\"Tempest\",\"Whirlwind\",\"Cyclone\",\"Hurricane\"]\tViolent winds\n",
    "[\"Goose\",\"Nature\",\"Tongue\",\"Shipton\"]\tMother ____\n",
    "[\"Winston\",\"Bollo\",\"Grodd\",\"Donkey Kong\"]\tFictional gorillas\n",
    "\n",
    "[\"Aggregate\",\"Replay\",\"Draw\",\"Leg\"]\tCup football terminology\n",
    "[\"Mark\",\"Catch\",\"Twig\",\"Spot\"]\tSynonyms for \"notice\"\n",
    "[\"Droopy\",\"Brain\",\"Astro\",\"Tramp\"]\tCartoon dogs\n",
    "[\"Dill\",\"Sage\",\"Basil\",\"Bay\"]\tAromatic herbs\n",
    "\n",
    "[\"Hand\",\"Dial\",\"Quartz\",\"Crystal\"]\tParts of a clock\n",
    "[\"George Washington\",\"Brooklyn\",\"Broadway\",\"Alexander Hamilton\"]\tBridges in New York\n",
    "[\"Hawker\",\"Embraer\",\"Bombardier\",\"Airbus\"]\tAeroplane brands\n",
    "[\"Donald\",\"Bentina\",\"Scrooge\",\"Louie\"]\tDuckTales characters\n",
    "\n",
    "[\"Beady\",\"Katie\",\"Emmy\",\"Esso\"]\tLetter + letter sounds\n",
    "[\"Olympique\",\"Bayern\",\"Borussia\",\"Real\"]\tChampions League winner prefixes\n",
    "[\"Methuselah\",\"Hannibal\",\"Odin\",\"Pope Benedict\"]\tAnthony Hopkins roles\n",
    "[\"Shudder\",\"Jerk\",\"Spasm\",\"Tic\"]\tTwitch\n",
    "\n",
    "[\"Man\",\"Card\",\"Coel\",\"Brain\"]\t-iac\n",
    "[\"Drive\",\"Fracture\",\"First Man\",\"The Notebook\"]\tRyan Gosling films\n",
    "[\"Diamonds\",\"We Found Love\",\"Umbrella\",\"Stay\"]\tRihanna songs\n",
    "[\"Tue\",\"Thu\",\"Fri\",\"Sat\"]\tDays of the week\n",
    "\n",
    "[\"Fenugreek\",\"Turmeric\",\"Mace\",\"Sumac\"]\tSpices\n",
    "[\"Flash\",\"Porcupine\",\"Wilhelm\",\"Soak\"]\tEnd in trees\n",
    "[\"Crimson\",\"Garnet\",\"Maroon\",\"Burgundy\"]\tShades of red\n",
    "[\"Creed\",\"Rocky\",\"Ali\",\"Million Dollar Baby\"]\tBoxing films\n",
    "\n",
    "[\"Flail\",\"Mace\",\"Dagger\",\"Estoc\"]\tHistorical weapons\n",
    "[\"Dragon Ball\",\"Naruto\",\"One Piece\",\"Bleach\"]\tJapanese manga series\n",
    "[\"Peanuts\",\"Archie\",\"Dilbert\",\"Blondie\"]\tComic strips\n",
    "[\"Pink\",\"Pegasus\",\"Stickleback\",\"Tier\"]\tBegin with \"fasten\" words\n",
    "\n",
    "[\"Azure\",\"Navy\",\"Cyan\",\"Teal\"]\tBlues\n",
    "[\"Wee\",\"Dinky\",\"Minute\",\"Slight\"]\tLittle\n",
    "[\"Ray\",\"Collateral\",\"Baby Driver\",\"Annie\"]\tJamie Foxx films\n",
    "[\"Dictionary\",\"Amen\",\"Scotch\",\"Pooh\"]\tFamous corners\n",
    "\n",
    "[\"Peacock\",\"Red Admiral\",\"Grayling\",\"Brimstone\"]\tButterflies\n",
    "[\"Huahua\",\"Lean\",\"Chester\",\"Nook\"]\tChi-\n",
    "[\"Scarab\",\"Cockchafer\",\"Firefly\",\"Weevil\"]\tBeetles\n",
    "[\"Older\",\"Faith\",\"Outside\",\"Mothers Pride\"]\tGeorge Michael songs\n",
    "\n",
    "[\"Shave\",\"Pare\",\"Peel\",\"Trim\"]\tReduce\n",
    "[\"Perceval\",\"Canning\",\"Major\",\"Baldwin\"]\tPrime Ministers\n",
    "[\"Dawn\",\"Break\",\"Darko\",\"Dancing\"]\tLast word of Patrick Swayze film titles\n",
    "[\"Hopscotch\",\"Statues\",\"Dodgeball\",\"Tag\"]\tPlayground games\n",
    "\n",
    "[\"Jerry\",\"Mrs Frisby\",\"Tag\",\"Mickey\"]\tFictional mice\n",
    "[\"Pouco\",\"Modicum\",\"Peu\",\"Wenig\"]\tA little in different languages\n",
    "[\"Goya\",\"Miró\",\"Gris\",\"Varo\"]\tSpanish painters\n",
    "[\"Trap\",\"Web\",\"Net\",\"Noose\"]\tThings you can get caught in\n",
    "\n",
    "[\"Kettle\",\"Skillet\",\"Griddle\",\"Marmite\"]\tTypes of cookware\n",
    "[\"Disclose\",\"Blab\",\"Reveal\",\"Divulge\"]\tGive out information\n",
    "[\"Been\",\"Leak\",\"Beat\",\"P\"]\tSound a bit like vegetables\n",
    "[\"Lukewarm\",\"Johnson\",\"Market\",\"Mattress\"]\tBegin with male names\n",
    "\n",
    "[\"Holidays\",\"Decorations\",\"Greetings\",\"Lights\"]\tChristmas ____\n",
    "[\"Date\",\"Srichaphan\",\"Bhupathi\",\"Osaka\"]\tAsian tennis players\n",
    "[\"Other Half\",\"Inamorata\",\"Consort\",\"Partner\"]\tSignificant others\n",
    "[\"Applause\",\"Drinks\",\"Golf\",\"Gunfire\"]\tThings that come in rounds\n",
    "\n",
    "[\"Transporter\",\"Beetle\",\"Polo\",\"Fox\"]\tVolkswagens\n",
    "[\"Panier\",\"Undined\",\"Anal duck\",\"Newton Gill\"]\tAnagrams of New Zealand cities\n",
    "[\"Doozy\",\"Corker\",\"Crackerjack\",\"Knockout\"]\tExcellent things\n",
    "[\"Bus\",\"Carer\",\"Hooer\",\"Haring\"]\tUS presidents missing 4th letter\n",
    "\n",
    "[\"Mamie\",\"Nancy\",\"Lady Bird\",\"Bess\"]\tFirst ladies of the US\n",
    "[\"Greit\",\"Paroled\",\"Penarth\",\"Lino\"]\tAnagrams of big cats\n",
    "[\"Play\",\"Drive\",\"Analytics\",\"Authenticator\"]\tGoogle services\n",
    "[\"Knows\",\"Waste\",\"Tow\",\"Heal\"]\tSounds a bit like a body part\n",
    "\n",
    "[\"Pupil\",\"Coltrane\",\"Cube\",\"Lambert\"]\tBegin with young animals\n",
    "[\"Waltz\",\"Malek\",\"Bean\",\"Wiseman\"]\tBond villains\n",
    "[\"Inner\",\"Atlantic\",\"Radio\",\"Garden\"]\t____ City\n",
    "[\"Kite\",\"Eagle\",\"Hobby\",\"Owl\"]\tBirds of prey\n",
    "\n",
    "[\"Station\",\"Off\",\"Book\",\"Ground\"]\tPlay____\n",
    "[\"Pimple\",\"Freckle\",\"Mole\",\"Spot\"]\tFacial features\n",
    "[\"Morpheus\",\"Tank\",\"Trinity\",\"Switch\"]\tCharacters in 'The Matrix'\n",
    "[\"Peterhouse\",\"Clare\",\"Magdalene\",\"Darwin\"]\tCambridge colleges\n",
    "\n",
    "[\"Cézanne\",\"Catania\",\"Truth\",\"Dilemma\"]\tEnd with a girl's name\n",
    "[\"Wave\",\"Voice\",\"Storm\",\"News\"]\tThings that break\n",
    "[\"Inc\",\"Vase\",\"Crino\",\"Stream\"]\t-line\n",
    "[\"Admiral\",\"Adidas\",\"Nike\",\"Puma\"]\tSportswear brands\n",
    "\n",
    "[\"Grunfeld\",\"Reti\",\"Sicilian\",\"Budapest\"]\tChess openings\n",
    "[\"Guard\",\"Breather\",\"Off\",\"Ulcer\"]\tMouth ____\n",
    "[\"Barley\",\"Rye\",\"Sorghum\",\"Fonio\"]\tGrains\n",
    "[\"Peso\",\"Real\",\"Bolívar\",\"Boliviano\"]\tSouth American currencies\n",
    "\n",
    "[\"Gammon\",\"Speck\",\"Bacon\",\"Presunto\"]\tPork products\n",
    "[\"Single\",\"Greenback\",\"Buck\",\"Simoleon\"]\t$1\n",
    "[\"Steer\",\"Drake\",\"Drone\",\"Bull\"]\tMale animals\n",
    "[\"Dram\",\"Yen\",\"Yuan\",\"Kip\"]\tAsian currencies\n",
    "\n",
    "[\"Fight\",\"Horn\",\"Finch\",\"Doze\"]\tBull____\n",
    "[\"Snooze\",\"Slumber\",\"Siesta\",\"Nap\"]\tSleep\n",
    "[\"Brush\",\"Trace\",\"Snip\",\"Share\"]\tHurry, when you remove the first letter\n",
    "[\"Bit\",\"Nip\",\"Smidge\",\"Iota\"]\tJust a little\n",
    "\n",
    "[\"Calvados\",\"Tequila\",\"Slivovitz\",\"Krupnik\"]\tAlcoholic drinks\n",
    "[\"Boxer\",\"Pomeranian\",\"Pointer\",\"Akita\"]\tBreeds of dog\n",
    "[\"Pomelo\",\"Grapefruit\",\"Yuzu\",\"Clementine\"]\tCitrus fruits\n",
    "[\"Turquoise\",\"Dalmatian\",\"Skeleton\",\"Amalfi\"]\tCoasts\n",
    "\n",
    "[\"Leviathan\",\"Kraken\",\"Umibozu\",\"Jormungandr\"]\tMythical sea monsters\n",
    "[\"Ghost\",\"Chipotle\",\"Poblano\",\"Jalapeno\"]\tPeppers\t\n",
    "[\"Xeno\",\"Neo\",\"Krypto\",\"Rado\"]\tNoble gases missing their last letter\t\n",
    "[\"Pewterer\",\"Cryptologist\",\"Navel\",\"Chancellor\"]\tThings you would find in a church\t\n",
    "\n",
    "[\"Yeoh\",\"Obama\",\"Visage\",\"Kwan\"]\tMichelles\t\n",
    "[\"Bottom\",\"Strange\",\"Top\",\"Charm\"]\tTypes of quark\t\n",
    "[\"Akron\",\"Dayton\",\"Columbus\",\"Kent\"]\tCities in Ohio\n",
    "[\"Black\",\"Streatham\",\"Clash\",\"Thiago Silva\"]\tDave songs\n",
    "\n",
    "[\"Winner\",\"Conqueror\",\"Vanquisher\",\"Champion\"]\tWinner\n",
    "[\"Cabriole\",\"Fondu\",\"Arabesque\",\"Coupé\"]\tBallet terms\n",
    "[\"Thyroid\",\"Pituitary\",\"Hypothalamus\",\"Adrenal\"]\tEndocrine glands\n",
    "[\"Minerva\",\"Juno\",\"Flora\",\"Fortuna\"]\tRoman goddesses\n",
    "\n",
    "[\"Angers\",\"Gap\",\"Nice\",\"Nancy\"]\tFrench towns/cities\n",
    "[\"Fifty\",\"Inductance\",\"Luxembourg\",\"Learner\"]\tCan be represented by L\n",
    "[\"Stone\",\"Broom\",\"Skip\",\"Third\"]\tCurling terminology\n",
    "[\"Hot\",\"Fit\",\"Foxy\",\"Dreamy\"]\tSlang for attractive\n",
    "\n",
    "[\"Blackness\",\"Glamis\",\"Balmoral\",\"Tantallon\"]\tScottish castles\n",
    "[\"Pongo\",\"Mowgli\",\"Simba\",\"Wart\"]\tDisney protagonists\n",
    "[\"Skeleton\",\"Smart\",\"Dimple\",\"Chip\"]\tTypes of key\n",
    "[\"Me\",\"Me More\",\"Kiss\",\"Me Thru The Phone\"]\t'Kiss ____' songs\n",
    "\n",
    "[\"Boom\",\"Hike\",\"Upswing\",\"Swell\"]\tIncreases\n",
    "[\"Bitcoin\",\"Dogecoin\",\"Ethereum\",\"Terra\"]\tCryptocurrencies\n",
    "[\"Mears\",\"Charles\",\"Davies\",\"Parlour\"]\tRays\n",
    "[\"Doozy\",\"Corker\",\"Crackerjack\",\"Knockout\"]\tExcellent things\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rest of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_connections_data(input_data):\n",
    "    games = input_data.strip().split('\\n\\n')\n",
    "    all_game_data = []\n",
    "\n",
    "    for game in games:\n",
    "        lines = [line.strip() for line in game.strip().split('\\n') if line.strip()]\n",
    "        game_entries = []\n",
    "        all_words = []\n",
    "\n",
    "        if len(lines) != 4:\n",
    "            print(lines, \"\\n\", game)\n",
    "            raise ValueError(\"Each game must contain exactly 16 words across 4 groups.\")\n",
    "\n",
    "        for line in lines:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "\n",
    "            words_str, reason = parts\n",
    "            words = [word.strip(' \"').lower() for word in words_str.strip('[]').split(\",\")]\n",
    "            all_words.extend(words)\n",
    "            game_entries.append({'words': words, 'reason': reason.strip()})\n",
    "\n",
    "        if len(all_words) != 16:\n",
    "            print(all_words, game)\n",
    "            raise ValueError(\"Each game must have exactly 16 words.\")\n",
    "        \n",
    "        random.shuffle(all_words)\n",
    "\n",
    "        game_data = {\n",
    "            'words': all_words,\n",
    "            'solution': {\n",
    "                'groups': game_entries\n",
    "            }\n",
    "        }\n",
    "\n",
    "        all_game_data.append(game_data)\n",
    "\n",
    "    return all_game_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = parse_connections_data(input_data)\n",
    "ds_len = len(ds)\n",
    "print(ds[0])\n",
    "print(ds_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Connect (DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Read the CSV data\n",
    "with open('wall_groups.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Process the data\n",
    "ds = []\n",
    "for i in range(0, len(rows), 4):\n",
    "    game_data = rows[i:i+4]\n",
    "    words = []\n",
    "    solution = {'groups': []}\n",
    "    \n",
    "    for entry in game_data:\n",
    "        clues = json.loads(entry['clues'])\n",
    "        clues = [clue.lower() for clue in clues]\n",
    "        words.extend(clues)\n",
    "        solution['groups'].append({\n",
    "            'words': clues,\n",
    "            'reason': entry['connection']\n",
    "        })\n",
    "    \n",
    "    if len(set(words)) != 16:\n",
    "        continue\n",
    "    \n",
    "    random.shuffle(words)\n",
    "    ds.append({\n",
    "        'words': words,\n",
    "        'solution': solution\n",
    "    })\n",
    "\n",
    "ds_len = len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Note: One W2V model used is conceptnet-numberbatch. Download the English-only compressed text file from here: https://github.com/commonsense/conceptnet-numberbatch?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzipped_file_path = 'numberbatch-en-19.08.txt.gz'\n",
    "decompressed_file_path = 'numberbatch-en-19.08.txt'\n",
    "\n",
    "with gzip.open(gzipped_file_path, 'rt', encoding='utf-8') as f_in:\n",
    "    with open(decompressed_file_path, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "# Import different models\n",
    "model_google = api.load('word2vec-google-news-300')\n",
    "model_glove = api.load('glove-wiki-gigaword-300')\n",
    "model_wiki = api.load('fasttext-wiki-news-subwords-300')\n",
    "model_conceptnet = KeyedVectors.load_word2vec_format(decompressed_file_path, binary=False)\n",
    "\n",
    "# Test: find similar words\n",
    "print(f\"GOOGLE NEWS: {model_google.most_similar('seattle')}\")\n",
    "print(f\"GLOVE: {model_glove.most_similar('seattle')}\")\n",
    "print(f\"WIKI: {model_wiki.most_similar('seattle')}\")\n",
    "print(f\"CONCEPTNET': {model_conceptnet.most_similar('seattle')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_google.similarity('squat', 'pushup'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess multi-word expressions (e.g. 'New York', 'push-up')\n",
    "def preprocess_word(word, model):\n",
    "  mwe = re.sub(r'[-\\s]', '_', word.lower())\n",
    "  \n",
    "  if mwe not in model:\n",
    "      mwe = re.sub(r'_', '', mwe)\n",
    "  \n",
    "  return mwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract words from ds[i]['words']\n",
    "def guess(model, words):\n",
    "  words = [preprocess_word(word, model) for word in words]\n",
    "  similarity_matrix = np.zeros((len(words), len(words)))\n",
    "  for i, word1 in enumerate(words):\n",
    "      for j, word2 in enumerate(words):\n",
    "          if word1 in model and word2 in model:\n",
    "              similarity_matrix[i, j] = model.similarity(word1, word2)\n",
    "          else:\n",
    "              similarity_matrix[i, j] = 0\n",
    "\n",
    "  similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "  _max = 0\n",
    "  argmax = 0\n",
    "  argword = \"\"\n",
    "  for idx, word in enumerate(words):\n",
    "    if type(similarity_df[word]) is pd.DataFrame:\n",
    "      print(similarity_df[word])\n",
    "    similar_words = similarity_df[word].sort_values(ascending=False)\n",
    "    if similar_words.iloc[1] > _max:\n",
    "      _max = similar_words.iloc[1]\n",
    "      argmax = idx\n",
    "      argword = similar_words.index[1]\n",
    "\n",
    "  build_list = [words[argmax], argword]\n",
    "\n",
    "  words_copy = words.copy()\n",
    "  for test_word in build_list:\n",
    "    if test_word not in words_copy:\n",
    "      return None\n",
    "    words_copy.remove(test_word)\n",
    "\n",
    "  sim_list = []\n",
    "  for test_word in words_copy:\n",
    "    similarities = []\n",
    "    for train_word in build_list:\n",
    "        if train_word in model and test_word in model:\n",
    "            similarity = model.similarity(train_word, test_word)\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            similarities.append(0)  # Handle words not in the model\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    sim_list.append(average_similarity)\n",
    "\n",
    "  index_of_highest_value = sim_list.index(max(sim_list))\n",
    "  build_list.append(words_copy[index_of_highest_value])\n",
    "\n",
    "  words_copy = words.copy()\n",
    "  for test_word in build_list:\n",
    "    if test_word not in words_copy:\n",
    "      return None\n",
    "    words_copy.remove(test_word)\n",
    "\n",
    "  sim_list = []\n",
    "  for test_word in words_copy:\n",
    "    similarities = []\n",
    "    for train_word in build_list:\n",
    "        if train_word in model and test_word in model:\n",
    "            similarity = model.similarity(train_word, test_word)\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            similarities.append(0)  # Handle words not in the model\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    sim_list.append(average_similarity)\n",
    "\n",
    "  index_of_highest_value = sim_list.index(max(sim_list))\n",
    "  build_list.append(words_copy[index_of_highest_value])\n",
    "\n",
    "  return build_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_similarity_matrix(model, words):\n",
    "    words = [preprocess_word(word, model) for word in words]\n",
    "    words = [word for word in words if word in model]\n",
    "    \n",
    "    similarity_matrix = {}\n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            if i < j:  # Avoid redundant computations\n",
    "                similarity_matrix[(word1, word2)] = model.similarity(word1, word2)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Extract words from ds[i]['words'] with fallback guesses\n",
    "# similarity_matrix: precomputed similarity matrix\n",
    "\n",
    "def guess_best_combination(model, words, similarity_matrix=None, lives=4):\n",
    "    if len(words) == 4:\n",
    "        return [list(words) * lives]\n",
    "    words = [preprocess_word(word, model) for word in words]\n",
    "    words = [word for word in words if word in model]\n",
    "\n",
    "    if len(words) < 4 or lives < 1:\n",
    "        return None\n",
    "\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix = compute_similarity_matrix(model, words)\n",
    "\n",
    "    all_combinations = list(combinations(words, 4))\n",
    "    scored_combinations = []\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        similarities = []\n",
    "        for i, word1 in enumerate(combination):\n",
    "            for j, word2 in enumerate(combination):\n",
    "                if i < j:\n",
    "                    similarities.append(similarity_matrix.get((word1, word2), similarity_matrix.get((word2, word1), 0)))\n",
    "\n",
    "        average_similarity = np.mean(similarities)\n",
    "        scored_combinations.append((combination, average_similarity))\n",
    "\n",
    "    # Sort combinations by average similarity in descending order\n",
    "    scored_combinations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return up to four attempts in descending order of similarity\n",
    "    top_guesses = [list(comb[0]) for comb in scored_combinations[:lives]]\n",
    "    return top_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_round(guess_list, solution):\n",
    "  right_count = [0, 0, 0, 0]\n",
    "  for final_word in guess_list:\n",
    "    for idx, group in enumerate(solution['groups']):\n",
    "      if final_word in group['words']:\n",
    "        right_count[idx] += 1\n",
    "  return max(right_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_google, model_glove, model_wiki, model_conceptnet]\n",
    "model_names = [\"Google News\", \"Glove\", \"Wikipedia\", \"ConceptNet\"]\n",
    "correct_idx = []\n",
    "for idx, model in enumerate(models):\n",
    "  print(f\"======== {model_names[idx]} ========\")\n",
    "  right_list = []\n",
    "  one_away_when = []\n",
    "  for i in range(ds_len):\n",
    "    guess_list = guess(model, ds[i]['words'])\n",
    "    if guess_list is not None:\n",
    "      score = eval_round(guess_list, ds[i]['solution'])\n",
    "      right_list.append(score)\n",
    "      if score == 4 and i not in correct_idx:\n",
    "        correct_idx.append(i)\n",
    "\n",
    "  print(f\"AVERAGE SCORE: {sum(right_list) / len(right_list)}\")\n",
    "  for i in range(1, 5):\n",
    "    print(f\"{i}: {right_list.count(i)}\")\n",
    "  print()\n",
    "print(f\"Number of Games with At Least One Good First Guess: {len(correct_idx)} / {ds_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(num_correct, strikes):\n",
    "    # Define multipliers and penalties\n",
    "    multipliers = [1, 2, 3, 3]\n",
    "    penalties = [1.0, 0.9, 0.75, 0.5, 0.25]\n",
    "\n",
    "    # Ensure the number of correct groups is within the valid range\n",
    "    if num_correct > 4:\n",
    "        num_correct = 4\n",
    "\n",
    "    # Calculate the total score\n",
    "    total_score = 0\n",
    "    for i in range(num_correct):\n",
    "        total_score += 1 * multipliers[i] * penalties[strikes]\n",
    "\n",
    "    return np.round(total_score, 2)\n",
    "\n",
    "# Example usage\n",
    "num_correct_1 = 4\n",
    "num_correct_2 = 4\n",
    "num_correct_3 = 2\n",
    "\n",
    "strikes_1 = 0\n",
    "strikes_2 = 1\n",
    "strikes_3 = 2\n",
    "\n",
    "print(\"All Correct with 0 strikes:\", calculate_score(num_correct_1, strikes_1))  # Output: 9.0\n",
    "print(\"All Correct with 1 strike:\", calculate_score(num_correct_2, strikes_2))   # Output: 8.1\n",
    "print(\"2 Correct Groups - 2 strikes:\", calculate_score(num_correct_3, strikes_3)) # Output: 2.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_google, model_glove, model_wiki, model_conceptnet]\n",
    "model_names = [\"Google News\", \"Glove\", \"Wikipedia\", \"ConceptNet\"]\n",
    "correct_idx = []\n",
    "multiplier = {4: 1.0, 3: 0.9, 2: 0.75, 1: 0.5, 0: 0.25}\n",
    "for idx, model in enumerate(models):\n",
    "  print(f\"======== {model_names[idx]} ========\")\n",
    "  right_list = []\n",
    "  correct_guesses = []\n",
    "  total_scores = []\n",
    "  one_away_when = []\n",
    "  for i in range(ds_len):\n",
    "    #print(\"I:\", i)\n",
    "    lives = 4\n",
    "    correct_count = 0\n",
    "    total_score = 0\n",
    "    options = ds[i]['words']\n",
    "    while lives > 0 and len(options) > 0:\n",
    "      #print(\"LEN:\", len(options))\n",
    "      guess_list = guess_best_combination(model, options, lives=lives)\n",
    "      #print(\"GUESS:\", guess_list)\n",
    "      if guess_list is None:\n",
    "        lives -= 1\n",
    "        continue\n",
    "      if guess_list is not None:\n",
    "        for guess in guess_list:\n",
    "          score = eval_round(guess, ds[i]['solution'])\n",
    "          if score == 4:\n",
    "            correct_count += 1\n",
    "            right_list.append(score)\n",
    "            options = [item for item in options if item not in guess]\n",
    "            if len(options) == 4:\n",
    "              correct_count += 1\n",
    "              options = []\n",
    "            break\n",
    "          lives -= 1\n",
    "          if guess == guess_list[-1] or lives == 0:\n",
    "            right_list.append(score)\n",
    "            break\n",
    "    correct_guesses.append(correct_count)\n",
    "    total_scores.append(calculate_score(correct_count, 4 - lives))\n",
    "    if correct_count == 4 and i not in correct_idx:\n",
    "      correct_idx.append(i)\n",
    "\n",
    "  print(f\"AVERAGE SCORE: {sum(correct_guesses) / len(correct_guesses)}\")\n",
    "  for i in range(0, 5):\n",
    "    print(f\"{i}: {correct_guesses.count(i)}\")\n",
    "  print(f\"Average Total Score: {sum(total_scores) / len(total_scores)} (Total: {sum(total_scores)})\")\n",
    "  print()\n",
    "print(f\"Number of Games with At Least One Complete Solve: {len(correct_idx)} / {ds_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_associations_from_conceptnet(word):\n",
    "    url = f\"http://api.conceptnet.io/c/en/{word.lower()}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return [edge['end']['label'] for edge in data['edges'] if 'Rel' in edge['rel']['label']]\n",
    "\n",
    "# Example usage\n",
    "associations = get_associations_from_conceptnet('devil')\n",
    "print(associations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_words(model, words, num_clusters=4):\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(word_vectors)\n",
    "\n",
    "    clusters = {i: [] for i in range(num_clusters)}\n",
    "    for i, word in enumerate(words):\n",
    "        if word in model:\n",
    "            cluster_label = kmeans.predict([model[word]])[0]\n",
    "            clusters[cluster_label].append(word)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_group_similarity(model, group, candidate):\n",
    "    similarities = [model.similarity(candidate, word) for word in group if candidate in model and word in model]\n",
    "    return sum(similarities) / len(similarities) if similarities else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_group_similarity(model_google, ['strange', 'down', 'charm'], 'up'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_similarity(models, word1, word2):\n",
    "    scores = [model.similarity(word1, word2) for model in models if word1 in model and word2 in model]\n",
    "    return sum(scores) / len(scores) if scores else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy==3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_named_entities(words):\n",
    "    doc = nlp(\" \".join(words))\n",
    "    return {ent.text: ent.label_ for ent in doc.ents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ensemble_similarity(models, 'red-sox', 'yankees'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode a word or phrase and get its embedding\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the last hidden state and mean pooling to get the sentence embedding\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    mean_embedding = embeddings.mean(dim=1)  # Mean pooling\n",
    "    return mean_embedding.squeeze()\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0)).item()\n",
    "\n",
    "# Updated function to find the best combination of words\n",
    "def guess_best_combination_bert(words):\n",
    "    word_embeddings = {}\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Obtain embeddings for each word/phrase\n",
    "            word_embeddings[word] = get_embedding(word)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {word}: {e}\")\n",
    "    \n",
    "    if len(word_embeddings) < 4:\n",
    "        raise ValueError(\"Not enough valid words to form a group of 4.\")\n",
    "    \n",
    "    best_combination = None\n",
    "    highest_average_similarity = -1\n",
    "\n",
    "    for combination in combinations(word_embeddings.keys(), 4):\n",
    "        similarities = []\n",
    "        for i, word1 in enumerate(combination):\n",
    "            for j, word2 in enumerate(combination):\n",
    "                if i < j:\n",
    "                    sim = cosine_similarity(word_embeddings[word1], word_embeddings[word2])\n",
    "                    similarities.append(sim)\n",
    "        \n",
    "        average_similarity = np.mean(similarities)\n",
    "        \n",
    "        if average_similarity > highest_average_similarity:\n",
    "            highest_average_similarity = average_similarity\n",
    "            best_combination = combination\n",
    "            print(highest_average_similarity, best_combination)\n",
    "\n",
    "    return list(best_combination) if best_combination else []\n",
    "\n",
    "# print(ds[0]['words'])\n",
    "result = guess_best_combination_bert([\"california\", \"wisconsin\", \"texas\", \"new york\", \"toaster\", \"bloom\", \"gone\", \"cheese\"])\n",
    "print(\"Best combination:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    mean_embedding = embeddings.mean(dim=1).squeeze(0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0][0]\n",
    "\n",
    "def find_most_similar_pair(word_embeddings):\n",
    "    max_similarity = -1\n",
    "    most_similar_pair = (None, None)\n",
    "    words = list(word_embeddings.keys())\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words)):\n",
    "            sim = calculate_similarity(word_embeddings[words[i]], word_embeddings[words[j]])\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                most_similar_pair = (words[i], words[j])\n",
    "\n",
    "    return most_similar_pair\n",
    "\n",
    "def expand_group(selected_words, remaining_words, word_embeddings):\n",
    "    max_similarity = -1\n",
    "    best_word = None\n",
    "\n",
    "    for word in remaining_words:\n",
    "        # Calculate average similarity between the current word and the selected words\n",
    "        similarities = [calculate_similarity(word_embeddings[word], word_embeddings[sw]) for sw in selected_words]\n",
    "        avg_similarity = np.mean(similarities)\n",
    "\n",
    "        if avg_similarity > max_similarity:\n",
    "            max_similarity = avg_similarity\n",
    "            best_word = word\n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_guess(words):\n",
    "    # Generate embeddings for each word\n",
    "    word_embeddings = {word: get_word_embedding(word) for word in words}\n",
    "\n",
    "    # Step 3: Find the pair of two words that are most similar to each other\n",
    "    word1, word2 = find_most_similar_pair(word_embeddings)\n",
    "    selected_words = [word1, word2]\n",
    "\n",
    "    # Step 4-5: Find the next most similar word to the current selection\n",
    "    remaining_words = [w for w in words if w not in selected_words]\n",
    "    for _ in range(2):  # Repeat twice to find 3rd and 4th words\n",
    "        best_word = expand_group(selected_words, remaining_words, word_embeddings)\n",
    "        selected_words.append(best_word)\n",
    "        remaining_words.remove(best_word)\n",
    "\n",
    "    return selected_words\n",
    "\n",
    "def eval_round(words, solution):\n",
    "    right_count = [0, 0, 0, 0]\n",
    "    for final_word in words:\n",
    "        for idx, group in enumerate(solution['groups']):\n",
    "            if final_word in group['words']:\n",
    "                right_count[idx] += 1\n",
    "    return max(right_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_list = []\n",
    "for i in range(ds_len):\n",
    "    if i > 0 and i % 100 == 0:\n",
    "        print(f\"Game {i}\")\n",
    "    if i == 300:\n",
    "        continue\n",
    "    words = ds[i]['words']\n",
    "    soln = ds[i]['solution']\n",
    "    optimal_guess = bert_guess(words)\n",
    "    score = eval_round(optimal_guess, soln)\n",
    "    right_list.append(score)\n",
    "\n",
    "print(f\"AVERAGE SCORE: {sum(right_list) / len(right_list)}\")\n",
    "print(\"========GAMES BY MAX NUM RIGHT ENTRIES========\")\n",
    "for i in range(1, 5):\n",
    "    print(f\"{i}: {right_list.count(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_best_combination_sbert(model, words):\n",
    "    # Generate embeddings for all words\n",
    "    word_embeddings = {word: model.encode(word, convert_to_tensor=True) for word in words}\n",
    "    \n",
    "    if len(word_embeddings) < 4:\n",
    "        return None\n",
    "    \n",
    "    top_combinations = []\n",
    "\n",
    "    for combination in combinations(word_embeddings.keys(), 4):\n",
    "        similarities = []\n",
    "        for i, word1 in enumerate(combination):\n",
    "            for j, word2 in enumerate(combination):\n",
    "                if i < j:\n",
    "                    sim = util.pytorch_cos_sim(word_embeddings[word1], word_embeddings[word2]).item()\n",
    "                    similarities.append(sim)\n",
    "        \n",
    "        average_similarity = np.mean(similarities)\n",
    "        \n",
    "        top_combinations.append((combination, average_similarity))\n",
    "\n",
    "    top_combinations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [list(combo[0]) for combo in top_combinations[:4]]\n",
    "\n",
    "# Example usage:\n",
    "result = guess_best_combination_sbert(model, ds[0]['words'])\n",
    "for guess in result:\n",
    "    print(\"Best combinations:\", guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds[300]['words'])\n",
    "print(sbert_model.encode(\"🐑\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_list = []\n",
    "correct_idx = []\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "for i in range(ds_len):\n",
    "  if i % 100 == 0:\n",
    "    print(f\"Game {i}\")\n",
    "  guess_list = guess_best_combination_sbert(sbert_model, ds[i]['words'])\n",
    "  best_guess = []\n",
    "  if guess_list is not None:\n",
    "    for guess in guess_list:\n",
    "      score = eval_round(guess, ds[i]['solution'])\n",
    "      best_guess.append(score)\n",
    "      if score == 4:\n",
    "        right_list.append(score)\n",
    "        if i not in correct_idx:\n",
    "          correct_idx.append(i)\n",
    "        break\n",
    "      elif guess == guess_list[-1]:\n",
    "        right_list.append(max(best_guess))\n",
    "        break\n",
    "\n",
    "print(f\"AVERAGE SCORE: {sum(right_list) / len(right_list)}\")\n",
    "for i in range(1, 5):\n",
    "  print(f\"{i}: {right_list.count(i)}\")\n",
    "print()\n",
    "print(f\"Number of Games with At Least One Good First Guess: {len(correct_idx)} / {ds_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...work in progress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_ds = load_dataset(\"wikimedia/structured-wikipedia\", \"20240916.en\")\n",
    "print(wiki_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
